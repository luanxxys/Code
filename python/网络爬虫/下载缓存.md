通过缓存结果避免重复下载

### 为链接爬虫添加缓存支持

```python
class Downloader:

    def __init__(self, delay=5, user_agent='wswp', proxies=None,num_retries=1, cache=None):
        self.throttle = Throttle(delay)
        self.user_agent = user_agent
        self.proxies = proxies
        self.num_retries = num_retries
        self.cache = cache

    def __call__(self, curl):
        result = None
        if self.cache:
            try:
                result = self.cache[url]
            except KeyError:
                # url is not available in cache
                pass
        else:
            if self.num_retries > 0 and \
            500 <= result['code'] < 600:
                result = None

    if result is None:
        # result was not loaded from cache
        # so still need to download
        self.throttle.wait(url)
        proxy = random.choice(self.proxies) if self.proxies else None
        headers = {'User.-agent': self.user_agent}
        result = self.download(url, headers, proxy, self.num_retries)
        if self.cache:
            # save result to cache
            self.cache[url] = result
        return result['html']

    def download(self, url, headers, proxy, num_retries, data=None):
        ....
        return {'html': html, 'code': code}
```

### 磁盘缓存

文件名字符规范：正则表达式

    re.sub('[^/0-9a-zA-Z\-.,;/_]', '_', url)

文件名长度限制

    file_name = '/'.join(segment[:255] for segmwnt in file_name.split('/'))

URL 边界保存（防止　/　冲突以保存到正确位置）

    urlparse.urlsplit()

具体实现

    ...

节省磁盘空间　--> 压缩

清理过期数据　--> 时间戳

### 数据库缓存

不受本地文件系统的限制

NoSQL

MongDB
